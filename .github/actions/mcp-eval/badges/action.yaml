name: "Generate MCP-Eval Badges"
description: "Generate badges from MCP-Eval JSON report using shields.io"
branding:
  icon: award
  color: green
inputs:
  report-path:
    description: "Path to the MCP-Eval JSON report file"
    required: true
  output-dir:
    description: "Directory to write generated badge files (optional, for caching)"
    default: "mcpeval-reports/badges"
    required: false
  tests-label:
    description: "Label text for the tests badge"
    default: "mcp-tests"
    required: false
  coverage-label:
    description: "Label text for the coverage badge"
    default: "mcp-cov"
    required: false
  upload-artifacts:
    description: "Upload badges as workflow artifacts"
    default: "false"
    required: false
  artifact-name:
    description: "Name for the uploaded badge artifacts"
    default: "mcpeval-badges"
    required: false
outputs:
  tests-badge-url:
    description: "URL for the tests badge"
    value: ${{ steps.generate.outputs.tests_badge_url }}
  coverage-badge-url:
    description: "URL for the coverage badge"
    value: ${{ steps.generate.outputs.coverage_badge_url }}
  tests-badge-path:
    description: "Path to the downloaded tests badge SVG (if output-dir is set)"
    value: ${{ steps.generate.outputs.tests_badge_path }}
  coverage-badge-path:
    description: "Path to the downloaded coverage badge SVG (if output-dir is set)"
    value: ${{ steps.generate.outputs.coverage_badge_path }}
  tests-passed:
    description: "Number of tests passed"
    value: ${{ steps.generate.outputs.tests_passed }}
  tests-total:
    description: "Total number of tests"
    value: ${{ steps.generate.outputs.tests_total }}
  coverage-percentage:
    description: "Tool coverage percentage"
    value: ${{ steps.generate.outputs.coverage_percentage }}
runs:
  using: "composite"
  steps:
    - name: Generate badge URLs from report
      id: generate
      shell: python
      run: |
        import json
        import os
        import urllib.request
        from pathlib import Path
        
        def get_color(percentage):
            """Get color name for shields.io based on percentage"""
            if percentage >= 90:
                return "brightgreen"
            elif percentage >= 75:
                return "green"
            elif percentage >= 50:
                return "yellowgreen"
            elif percentage >= 25:
                return "orange"
            else:
                return "red"
        
        # Parse report
        report_path = "${{ inputs.report-path }}"
        
        try:
            with open(report_path, 'r') as f:
                report = json.load(f)
            
            # Calculate test results
            summary = report.get("summary", {})
            total_tests = (summary.get("total_decorator_tests", 0) + 
                          summary.get("total_dataset_cases", 0))
            passed_tests = (summary.get("passed_decorator_tests", 0) + 
                           summary.get("passed_dataset_cases", 0))
            pass_rate = (passed_tests / total_tests * 100.0) if total_tests > 0 else 0.0
            
            # Calculate tool coverage
            available_tools = set()
            used_tools = set()
            
            # Process decorator tests
            for test in report.get("decorator_tests", []) or []:
                metrics = test.get("metrics", {})
                tool_coverage = metrics.get("tool_coverage", {})
                for server_name, coverage in tool_coverage.items():
                    if isinstance(coverage, dict):
                        for tool in coverage.get("available_tools", []):
                            available_tools.add(f"{server_name}:{tool}")
                        for tool in coverage.get("used_tools", []):
                            used_tools.add(f"{server_name}:{tool}")
            
            # Process dataset reports
            for dataset in report.get("dataset_reports", []) or []:
                for case in dataset.get("results", []) or []:
                    metrics = case.get("metrics", {})
                    tool_coverage = metrics.get("tool_coverage", {})
                    for server_name, coverage in tool_coverage.items():
                        if isinstance(coverage, dict):
                            for tool in coverage.get("available_tools", []):
                                available_tools.add(f"{server_name}:{tool}")
                            for tool in coverage.get("used_tools", []):
                                used_tools.add(f"{server_name}:{tool}")
            
            coverage_pct = (len(used_tools) / len(available_tools) * 100.0) if available_tools else 0.0
            
        except Exception as e:
            print(f"Warning: Failed to parse report: {e}")
            # Default values on error
            passed_tests = 0
            total_tests = 0
            pass_rate = 0.0
            coverage_pct = 0.0
        
        # Generate shield.io badge URLs
        tests_label = "${{ inputs.tests-label }}".replace("-", "--").replace("_", "__")
        coverage_label = "${{ inputs.coverage-label }}".replace("-", "--").replace("_", "__")
        
        tests_message = f"{passed_tests}/{total_tests}"
        tests_color = get_color(pass_rate)
        tests_url = f"https://img.shields.io/badge/{tests_label}-{tests_message}-{tests_color}"
        
        coverage_message = f"{int(round(coverage_pct))}%25"  # %25 is URL-encoded %
        coverage_color = get_color(coverage_pct)
        coverage_url = f"https://img.shields.io/badge/{coverage_label}-{coverage_message}-{coverage_color}"
        
        # Write outputs
        with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
            f.write(f"tests_badge_url={tests_url}\n")
            f.write(f"coverage_badge_url={coverage_url}\n")
            f.write(f"tests_passed={passed_tests}\n")
            f.write(f"tests_total={total_tests}\n")
            f.write(f"coverage_percentage={int(round(coverage_pct))}\n")
        
        print(f"Generated badge URLs:")
        print(f"  Tests: {tests_url}")
        print(f"  Coverage: {coverage_url}")
        
        # Optionally download badges to files
        output_dir = "${{ inputs.output-dir }}"
        if output_dir:
            output_path = Path(output_dir)
            output_path.mkdir(parents=True, exist_ok=True)
            
            try:
                # Download tests badge
                tests_path = output_path / "tests.svg"
                with urllib.request.urlopen(tests_url) as response:
                    tests_path.write_bytes(response.read())
                
                # Download coverage badge  
                coverage_path = output_path / "coverage.svg"
                with urllib.request.urlopen(coverage_url) as response:
                    coverage_path.write_bytes(response.read())
                
                with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                    f.write(f"tests_badge_path={tests_path.resolve()}\n")
                    f.write(f"coverage_badge_path={coverage_path.resolve()}\n")
                
                print(f"Downloaded badges to {output_path}")
            except Exception as e:
                print(f"Warning: Failed to download badges: {e}")

    - name: Upload badge artifacts
      if: ${{ inputs.upload-artifacts == 'true' && inputs.output-dir != '' }}
      uses: actions/upload-artifact@v4
      with:
        name: ${{ inputs.artifact-name }}
        path: ${{ inputs.output-dir }}
        retention-days: 14
        if-no-files-found: warn